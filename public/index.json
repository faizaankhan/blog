[{"categories":["Tech","Guest"],"content":"Introduction to GraphQL from REST API Perspective","date":"2021-05-01","objectID":"/guest-2-graphql/","tags":["GraphQL"],"title":"An Introduction to GraphQL for REST API Developers","uri":"/guest-2-graphql/"},{"categories":["Tech","Guest"],"content":"Why GraphQL? Before jumping into GrapQl, let us analyze a scenario for REST API. In REST APIs, we have resources. Each resource has an ID and methods to create, update, delete. Consider a situation; we have three models, company, employee, and task. A company has many employees, has many tasks, a task has many employees. And also, let’s say each task has a supervisor who is an employee. We can assume that we have GET, POST, PUT, PATCH, DELETE supported endpoints for all the resources. Scenario 1: Get company information for an ID. Call company endpoint with Method type GET and ID in parameters. Scenario 2: Get company name for ID. Scenario 1 works fine, but we will still be getting all data, and we’ll have to filter data at the client-side, which is not generally ideal even if data is not sensitive. One solution is to add an extra parameter in the GET function to tell what information we need, and our server can send only those data. i.e., { “only”: [“name”]} Scenario 3: Get Employee details, the task he is working on, and the task details. One option we can see here is to call GET for the ID, look for Task ID in the response make another request to get supervisor ID from task ID. Send a nested response from the API itself. For the above three scenarios, there are many exciting approaches in REST itself. Still, as the size and complexity of the application grow, you will be finding yourself writing some dedicated specific purpose APIs. ","date":"2021-05-01","objectID":"/guest-2-graphql/:1:0","tags":["GraphQL"],"title":"An Introduction to GraphQL for REST API Developers","uri":"/guest-2-graphql/"},{"categories":["Tech","Guest"],"content":"How GraphQL solves the problem. GraphQL gives you an option to write query in the frontend. So the concept is requester of data will query the data they need to curate it, instead of a controller doing it for them. It doesn’t have an easy learning curve as REST API, but once you are comfortable with it and the project boilerplate is ready, it will be much easier to build upon. ","date":"2021-05-01","objectID":"/guest-2-graphql/:2:0","tags":["GraphQL"],"title":"An Introduction to GraphQL for REST API Developers","uri":"/guest-2-graphql/"},{"categories":["Tech","Guest"],"content":"Basic Idea of implementing a GraphQL Backend. In GraphQL, you’ll have only one endpoint. Your frontend application will always request to that single endpoint with a GQL. With the GQL ( Graph Query Language), the backend will identify what data is needed and send it. The server should be able to understand GQL; else, it will not understand the request. So we will have to add the capability to the server. To do this, we need to add a GraphQL Implementation to our backend. For NodeJs, Apollo is an option; for Rails, there is a gem called graphql. Feel free to find the perfect graphql implementation for your backend; there are many. ","date":"2021-05-01","objectID":"/guest-2-graphql/:3:0","tags":["GraphQL"],"title":"An Introduction to GraphQL for REST API Developers","uri":"/guest-2-graphql/"},{"categories":["Tech","Guest"],"content":"You’ll need a minimum of three steps to have basic crud. Define Schema: This part is just like defining models. It will let your graphql engine know what data are available. It will feel repetitive as we already have models with similar code, but it will be worth it once it’s done. Write Query Resolvers: In this step, you will how to get the data. It is basically like the get method of the controller but a bit complex to implement because you may want to generalize things to handle joins, and single data and array of data, etc. Write Mutation Resolvers: This is same as Query Resolvers, but instead, for fetching data, it will be for adding and editing data. After these steps, you can test it using GraphiQl, just like PostMan for REST. ","date":"2021-05-01","objectID":"/guest-2-graphql/:3:1","tags":["GraphQL"],"title":"An Introduction to GraphQL for REST API Developers","uri":"/guest-2-graphql/"},{"categories":["Tech","Guest"],"content":"Benifit of using GraphQL This all feels like a lot of work compared to developing simple APIs. So there should be some benefits to adopt graphQl. You’ll always fetch the data you need. You don’t have to use an API that sends some extra data that you don’t need. Option to batch requests from the front end. As all requests are directed to a single endpoint and contain queries in the same language, these can be clubbed in a single request, and GraphQL can handle multiple queries in one request. If you are developing SPA, using component-based framework batching will be helpful as it can fetch data for each component at once, so a user won’t be waiting for half of the page to load. Many GraphQL Client libraries have this inbuilt, and some can also split if a query is too big for the first contentful paint. As the backends schema matures, the developer will find touching backend code less and less as most of the front data needs can be handled by GQL. Much dynamic UI can be built without the need for API to handle every minor thing. There are some other features to look for, such as subscriptions, authorizations, validation, pagination, etc., which will be helpful in the development lifecycle. I’ll highly recommend getting an overview of the fundamentals from https://graphql.org/learn/ and judge if it’s a viable option for your current or next project. Guest Post Shoutout to my friend Vikas for finally giving me this writeup :), I haven’t got a chance to use GraphQL yet, but I see good enough reason to explore it soon. I hope you too are up for the same. Also, Vikas has been writing blogs for some time, do check out his page geekyhub, there is some brilliant content in there. ","date":"2021-05-01","objectID":"/guest-2-graphql/:4:0","tags":["GraphQL"],"title":"An Introduction to GraphQL for REST API Developers","uri":"/guest-2-graphql/"},{"categories":["Tech"],"content":"General talk on what Kafka is.","date":"2021-03-06","objectID":"/tech-logs-1/","tags":["Log Management","Technical Articles","Kafka"],"title":"Curious about Kafka?","uri":"/tech-logs-1/"},{"categories":["Tech"],"content":" In strawman terms, Kafka is a messaging system which can be used across different systems to communicate. In the below image you can see that Kafka acts as a message bus in between web apps, microservices, analytics apps, databases. Various services contacting to each other with Kafka ","date":"2021-03-06","objectID":"/tech-logs-1/:0:0","tags":["Log Management","Technical Articles","Kafka"],"title":"Curious about Kafka?","uri":"/tech-logs-1/"},{"categories":["Tech"],"content":"We say Kafka is an open-source event stream processing platform. Kafka stores data in streams of continuous records which can then be processed by different systems. Event Streaming ","date":"2021-03-06","objectID":"/tech-logs-1/:1:0","tags":["Log Management","Technical Articles","Kafka"],"title":"Curious about Kafka?","uri":"/tech-logs-1/"},{"categories":["Tech"],"content":"So what’s an event here? Something happened !!! What ? -\u003e A change in the state of some data. Event is a lightweight representation of “something happened”. We have been looking at events perspective for so long in frontend, mouse click event, hover event, and so on, and here we are starting to look through the same glass in a larger field. We have a general structure for these events: What changed? : Key What is the change? : Value When did the change happen? Timestamp Who did the change? User Stamp The event can then be distributed to those interested on it. They can be any system, like an email notification service for a new follow request on Insta will be interested on a follow request event. ","date":"2021-03-06","objectID":"/tech-logs-1/:1:1","tags":["Log Management","Technical Articles","Kafka"],"title":"Curious about Kafka?","uri":"/tech-logs-1/"},{"categories":["Tech"],"content":"What’s an Event Stream? A small river is called a stream. I had one on my way to school, so I like to imagine streams in that way, mashup or nature and tech with nostalgia. If you are of the YouTube age, you already know what a stream is. Moving ahead from PJs, Stream represents a flow of something happening over a period of time, here “data”. Data being available to you over a span of time, which doesn’t have a potential limit or any finite value. An event stream consists of immutable data, where new events are added but previous ones can’t be changed. ","date":"2021-03-06","objectID":"/tech-logs-1/:1:2","tags":["Log Management","Technical Articles","Kafka"],"title":"Curious about Kafka?","uri":"/tech-logs-1/"},{"categories":["Tech"],"content":"Event Streaming Event Streaming is the constant flow of events where each event contains enough information to reflect the change in state. It allows the processing of data to occur in real-time (data in motion). Streaming data is unbounded, meaning it has no real beginning and no real end. They are processed as they occur and are managed accordingly. Event streaming is used for real-time processing and analysis of state changes. In Kafka we can persist event streams supporting the rebuilding of the state through replaying events in the order they were received. ","date":"2021-03-06","objectID":"/tech-logs-1/:1:3","tags":["Log Management","Technical Articles","Kafka"],"title":"Curious about Kafka?","uri":"/tech-logs-1/"},{"categories":["Tech"],"content":"We say Kafka is an open-source DISTRIBUTED event stream processing platform. Kafka works as a cluster of one or more nodes that can live in different Datacenters. We can distribute data / load across different nodes in the Kafka Cluster, and it is inherently scalable, available, and fault-tolerant. Just in case you are unsure of what a distributed system is: In a distributed system we are slicing off a work into pieces and distributing it across machines. You have components of a system spread across different machines or even networks. And a cluster is just a group of nodes (computers/systems), which are dynamic. These nodes can increase or decrease in number as per some constraints or manually. Kafka Cluster ","date":"2021-03-06","objectID":"/tech-logs-1/:2:0","tags":["Log Management","Technical Articles","Kafka"],"title":"Curious about Kafka?","uri":"/tech-logs-1/"},{"categories":["Tech"],"content":"Broker In the last image, you would have noticed the mention of a Broker. Brokers are nodes in a Kafka cluster running Kafka servers, which are responsible for storing the events. They allow the push and pull of events from them. ","date":"2021-03-06","objectID":"/tech-logs-1/:2:1","tags":["Log Management","Technical Articles","Kafka"],"title":"Curious about Kafka?","uri":"/tech-logs-1/"},{"categories":["Tech"],"content":"We say Kafka is an opensource distributed event stream processing platform based on the PUBLISHER-SUBSCRIBER model. Kafka works in the pub/sub method. Where it allows publishers to push into topics and subscribers to consume from a topic. Publisher Subscriber Model ","date":"2021-03-06","objectID":"/tech-logs-1/:3:0","tags":["Log Management","Technical Articles","Kafka"],"title":"Curious about Kafka?","uri":"/tech-logs-1/"},{"categories":["Tech"],"content":"Topic Kafka Topics are a bunch of similar events stored together in Kafka Brokers. Kafka records are organized into topics. Producer applications write data to topics and consumer applications read from topics. Records published to the cluster stay in the cluster until a configurable retention period has passed by. Kafka Topic ","date":"2021-03-06","objectID":"/tech-logs-1/:3:1","tags":["Log Management","Technical Articles","Kafka"],"title":"Curious about Kafka?","uri":"/tech-logs-1/"},{"categories":["Tech"],"content":"Publishers An application that publishes a message to a topic (Generally called channel in pub/sub). It does not know the destination of the event, but only where it needs to store this event. Generally you would use the event key to select a topic to store your event into. ","date":"2021-03-06","objectID":"/tech-logs-1/:3:2","tags":["Log Management","Technical Articles","Kafka"],"title":"Curious about Kafka?","uri":"/tech-logs-1/"},{"categories":["Tech"],"content":"Subscribers / Consumers An application that registers itself to a topic to receive the events. It does not know the source of the message. Consumers label themselves with a consumer group name, and each record published to a topic is delivered to one consumer instance within each subscribing consumer group. Consumer instances can be in separate processes or on separate machines. Kafka Consumers ","date":"2021-03-06","objectID":"/tech-logs-1/:3:3","tags":["Log Management","Technical Articles","Kafka"],"title":"Curious about Kafka?","uri":"/tech-logs-1/"},{"categories":["Tech"],"content":"Kafka is an opensource distributed event stream processing platform based on the pub/sub model which stores events using LOGS. One of the key elements in Kafka is the usage of the log data structure. Kafka stores events in the form of Logs in Kafka topics. Kafka Cluster with Consumers \u0026 Producers ","date":"2021-03-06","objectID":"/tech-logs-1/:4:0","tags":["Log Management","Technical Articles","Kafka"],"title":"Curious about Kafka?","uri":"/tech-logs-1/"},{"categories":["Tech"],"content":"What are logs? Not to confuse with application logs or to confuse with it. A log is perhaps the simplest possible storage abstraction. It is an append-only, totally-ordered sequence of records ordered by time. Records are appended to the end of the log, and reads proceed left-to-right. Each entry is assigned a unique sequential log entry number. ","date":"2021-03-06","objectID":"/tech-logs-1/:4:1","tags":["Log Management","Technical Articles","Kafka"],"title":"Curious about Kafka?","uri":"/tech-logs-1/"},{"categories":["Tech"],"content":"What’s the zookeeper? Zookeeper works as the central configuration and consensus management system for Kafka. It tracks the brokers, topics, and partition assignment, leader election, basically all the metadata about the cluster. ZooKeeper is used to notify producers and consumers about the presence of any new broker in the Kafka system or about the failure of any broker in the Kafka system. ","date":"2021-03-06","objectID":"/tech-logs-1/:4:2","tags":["Log Management","Technical Articles","Kafka"],"title":"Curious about Kafka?","uri":"/tech-logs-1/"},{"categories":["Tech"],"content":"What are people using it for? Here’s a list directly from the Kafka docs. To process payments and financial transactions in real-time, such as in-stock exchanges, banks, and insurances. To track and monitor cars, trucks, fleets, and shipments in real-time, such as in logistics and the automotive industry. To continuously capture and analyze sensor data from IoT devices or other equipment, such as in factories and wind parks. To collect and immediately react to customer interactions and orders, such as in retail, the hotel and travel industry, and mobile applications. To monitor patients in hospital care and predict changes in condition to ensure a timely treatment in emergencies. To connect, store, and make available data produced by different divisions of a company. To serve as the foundation for data platforms, event-driven architectures, and microservices. ","date":"2021-03-06","objectID":"/tech-logs-1/:4:3","tags":["Log Management","Technical Articles","Kafka"],"title":"Curious about Kafka?","uri":"/tech-logs-1/"},{"categories":["Tech"],"content":"References Here are a few links this article takes inspiration from. https://kafka.apache.org/documentation/#gettingStarted https://dzone.com/articles/a-begineers-approach-to-quotkafkaquot https://docs.confluent.io/platform/current/kafka/introduction.html https://robertleggett.blog/2020/03/02/choosing-event-streaming-or-messaging-for-your-architecture/ https://kafka.apache.org/intro https://www.cloudkarafka.com/blog/2016-11-30-part1-kafka-for-beginners-what-is-apache-kafka.html https://medium.com/swlh/understanding-kafka-a-distributed-streaming-platform-9a0360b99de8 https://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying ","date":"2021-03-06","objectID":"/tech-logs-1/:5:0","tags":["Log Management","Technical Articles","Kafka"],"title":"Curious about Kafka?","uri":"/tech-logs-1/"},{"categories":["Tech"],"content":"I am going to talk about how one tests the application controller methods","date":"2020-10-17","objectID":"/tech-rails-rspec/","tags":["Rails","Rspec"],"title":"Writing specs for application controller methods using Rspec's anonymous controller","uri":"/tech-rails-rspec/"},{"categories":["Tech"],"content":"Intro Hi, today I am writing about Anonymous controllers of RSpec. If you are here, you must be knowing how important it is to write unit tests for your application code, in my case, tests have saved me a lot of time from screwing up my applications. In this post I want to take a couple of scenarios, and explain how to test your controller methods using Anonymous controllers. ","date":"2020-10-17","objectID":"/tech-rails-rspec/:1:0","tags":["Rails","Rspec"],"title":"Writing specs for application controller methods using Rspec's anonymous controller","uri":"/tech-rails-rspec/"},{"categories":["Tech"],"content":"Scenario I got an Application Controller on-base inheriting from ActionController::API, and a namespaced controller, which is Api::Version6::ApiController, inheriting from ApplicationController, from where all my controllers for a specific API version inherit. I use them like everyone else to have dry code, common crud boilerplate, and error handlings, but have had ignored testing the dry methods in the past in these controllers. Testing any action callbacks becomes a mystic task sometimes, and I have seen a lot of people including me ending up writing a controller test calling the action of some other controller which inherits from the above controllers, and testing if the callback works, now this does solve the purpose if it’s just about writing a test somehow, but the strategy hides down the test for the callback into some random place, and it becomes almost useless. Let’s see how we can unit test our action callback methods, or even a helper method you have in your controllers taking example for the above two scenarios. ","date":"2020-10-17","objectID":"/tech-rails-rspec/:2:0","tags":["Rails","Rspec"],"title":"Writing specs for application controller methods using Rspec's anonymous controller","uri":"/tech-rails-rspec/"},{"categories":["Tech"],"content":"Scenario 1 Testing my Application controllers error handling. Here I have a small piece of code where I can send a 401 to the caller when there’s a failure in JWT authentication. class ApplicationController \u003c ActionController::API rescue_from JWT::VerificationError, with: :unauthorized_request private def unauthorized_request render status: :unauthorized end end Now testing the unauthorized_request method here is my goal, but how?. I can write a test for something like a SessionsController’s login method, send a data which can raise the verification error, and test if I get a 401 response. But would that be enough?, or good practice? I can test the behavior directly without calling some real action. Anonymous Controller is like dummy controllers that inherit from the described class, and allows us to add any dummy action to them. Rspec provides us with a method called controller in which we can pass a block with the actions we require. We are free to define any of the seven standard crud actions. # spec/controllers/application_controller_spec.rb RSpec.describe ApplicationController, type: :controller do controller do # Defining a dummy action for an anynomous controller which inherits from the described class. def index puts controller_name raise JWT::VerificationError end end describe \"handling jwt validation errors\" do it \"should return status unauthorized\" do get :index expect(response).to have_http_status(:unauthorized) end end end In the above test, I have defined an index method that raises the error I need to test the application controller for if it’s handled or not. I have left a puts controller_name there, to show you that you will get anonymous as the controller name. So this looks simple, you can go ahead and test your dry codes. All the before action, after action and error handling will also be easy to test. ","date":"2020-10-17","objectID":"/tech-rails-rspec/:3:0","tags":["Rails","Rspec"],"title":"Writing specs for application controller methods using Rspec's anonymous controller","uri":"/tech-rails-rspec/"},{"categories":["Tech"],"content":"Scenario 2 class Api::Version6::ApiController \u003c ApplicationController before_action :store_current_user_in_thread, only: [:custom_action] private def store_current_user_in_thread Current.user = params[:user_id] end end In this scenario, the conditions are slightly different, straight away implementation of the anonymous controller method won’t work for us here, as we have a custom action, which isn’t rails standard action, and we have a namespaced controller. All these things make writing the test here a bit mind-boggling until you know how the anonymous thing works. What's Current.user in the above code? Current is a model inheriting from ActiveSupport::CurrentAttributes, it lets you store thread isolated attributes. We generally use a Thread or this abstraction to store the current user, such that it’s available across model callbacks etc. If interested, you can see the API docs here Issue 1 - Namespaced controller: The RSpec method will find it hard to understand the described class and create the anonymous controller inheriting it since it’s a namespaced controller. What you need to do here, is that you need to define that and send it to the RSpec’s controller method as a parameter. As you can see, I have defined a DummyController in the spec below, which inherits from our ApiController. This will let us have the anonymous controller of our need as well as define the route in spec. Issue 2 - Custom action: For handling this, we will draw our own route to the custom_action, in the before the block in RSpec, yes, it’s as simple as that. # spec/controllers/api/version6/application_controller_spec.rb module Api module Version6 class DummyController \u003c Api::Version6::ApiController; end end end describe Api::Version6::ApiController, type: :controller do controller Api::Version6::DummyController do def custom_action render nothing: true end end before(:each) do routes.draw do namespace :api do namespace :version6 do get 'custom_action' =\u003e 'dummy#custom_action' end end end end context \"dummy controller\" do it \"should inherit from the api controller\" do expect(controller).to be_a_kind_of(Api::Version6::ApiController) end end context \"#store_current_user_in_thread\" do let!(:user) { create(:user) } it \"should set Current.user to nil when no user id in params\" do get :custom_action expect(Current.user).to eq(nil) end it \"should assign current user to current attributes thread when user id in params\" do get :custom_action, params: { user_id: user.id } expect(Current.user).to eq(user) end end end So, now you know you can define any dummy action in the RSpec, have an anonymous controller inheriting you dry controller codes, and can define any kind of route on the go, and test them. Good luck. ","date":"2020-10-17","objectID":"/tech-rails-rspec/:4:0","tags":["Rails","Rspec"],"title":"Writing specs for application controller methods using Rspec's anonymous controller","uri":"/tech-rails-rspec/"},{"categories":["Tech"],"content":"In this post I intend to provide the readers with basic concepts they need to start working with elasticsearch, and then carve their way to understanding it deeply themselves while using it","date":"2020-10-02","objectID":"/tech-elasticsearch-1/","tags":["Technical Articles","Elasticsearch"],"title":"Getting familiar with Elasticsearch","uri":"/tech-elasticsearch-1/"},{"categories":["Tech"],"content":"Intro Search has become such an extended part of our daily life. Before starting to write this post, I just searched for my fav indie artist Prateek Kuhad on Spotify, plugged in my headphones and here I am making keystrokes with the flow of Dil Beparwah 🙂. With a happy mood what I would like to convey is if you are building a web app, you gotta have the search features. So without boring you more let me dive into what this post is about. If you have a dynamic web app with a good amount of data size which you are looking at handling, and you are planning at adding features for just search or even more with filters, sorts, analytics, having a dedicated search service solves your problem to a great amount. Your databases won’t be busy with all the search kind of requests, and you will also provide your users with a huge difference in search result time. A dedicated search engine for an application in the present-day is much of a necessity for most use cases with a large user base or data. In this post I will take you through the base of elasticsearch and leave you at how we can use it, after that the Journey is all yours 🙂. ","date":"2020-10-02","objectID":"/tech-elasticsearch-1/:1:0","tags":["Technical Articles","Elasticsearch"],"title":"Getting familiar with Elasticsearch","uri":"/tech-elasticsearch-1/"},{"categories":["Tech"],"content":"What is a Document Store? You must have heard of Document data models at some point of time, or in the DynamoDB post where I talked about NoSql databases a bit. Document data models are non-relational and non-normalized flat datastore where you store your data in the form of keys and values in JSON, XML, YAML and alike formats. Document store points to a semi-structured data storage solution where we store document data. And a distributed document store is the one which has document stores across nodes in a cluster. (if you are unfamiliar with the words nodes and cluster, just understand it like a node is a machine in a network with a lot of machines which is called a cluster) Distributed Document Store Sketch ","date":"2020-10-02","objectID":"/tech-elasticsearch-1/:2:0","tags":["Technical Articles","Elasticsearch"],"title":"Getting familiar with Elasticsearch","uri":"/tech-elasticsearch-1/"},{"categories":["Tech"],"content":"Lucene Elasticsearch is built on top of Lucene, thus let us take a moment describing what it is. Lucene is a powerful, built-for-purpose full-text search library. A full-text search is a technique for search engines where they allow us to search textual content in a document. Lucene takes a raw stream of characters, bundles them into tokens, and persists them as terms in an index(assume index as a table of contents, we will come to it very soon). It can quickly query that index and provide the ordered results. By using Lucene directly in our applications, or as part of a server, we can perform full-text searches in real-time over gigabytes of content. We also get custom analysis and scoring techniques, using which we can improve the relevance of results. Tokens: Data parsing in Lucene extracts out individual units like words from a raw stream of characters and stores them in indexes, these individual units are called tokens. They are occurrences of these words/units. Terms: Stored tokens are called terms, and they are unit of search, it consists of the text as a string and the field in the document where it was found. You can think of this all by thinking of yourself as Lucene, and assume you receive a lot of data from your input senses, one of your senses is the vision, and you are having a constant stream of data received in your mind. While you are moving across your house, and focussing on what’s kept at what place, you are processing these data streams, and creating an index of where the phone is, where the spoons are, and where the comb is. Now when you require one of them like the comb, your brain quickly looks up at this index, and lets you know that comb is in your dressing table. Tokens and terms are simple but really good to understand. ","date":"2020-10-02","objectID":"/tech-elasticsearch-1/:3:0","tags":["Technical Articles","Elasticsearch"],"title":"Getting familiar with Elasticsearch","uri":"/tech-elasticsearch-1/"},{"categories":["Tech"],"content":"Inverted Index and Forward Index An index is like an address book, instead of telling here a person is, it tells you where a data is, and thus helps you to fetch that data quickly. So one can efficiently retrieve records using indexes. Inverted Index is a data structure that stores a mapping from words to their documents. Slow Indexing but Fast Search. Words Chapters Hello Chapter 1 Coffee Chapter 2, Chapter 3 Greetings Chapter 1 Forward index is a data structure that stores mapping from documents to words. Fast indexing but slow search. Chapters Words Chapter 1 Hello, mello, kello Chapter 2 Tea, Coffee, Biscuits Chapter 3 Apple, Banana, Coffee You can take a moment to see the index tables and think what should have been a better choice for the elasticsearch developers. ","date":"2020-10-02","objectID":"/tech-elasticsearch-1/:4:0","tags":["Technical Articles","Elasticsearch"],"title":"Getting familiar with Elasticsearch","uri":"/tech-elasticsearch-1/"},{"categories":["Tech"],"content":"Elasticsearch Elasticsearch is a Lucene based distributed search and analytics engine. Free and open-source distributed inverted index Build on top of Apache Lucene Cross-platform Near real-time Indexed data can respond to complex search queries, filters and aggregations Is document-based Is scalable Data in JSON format Pluggable API endpoint ","date":"2020-10-02","objectID":"/tech-elasticsearch-1/:5:0","tags":["Technical Articles","Elasticsearch"],"title":"Getting familiar with Elasticsearch","uri":"/tech-elasticsearch-1/"},{"categories":["Tech"],"content":"1. Cluster: An elasticsearch cluster in general consists of loosely or tightly connected nodes, which together works as one unit. Cluster virtually describes the set of nodes running on it, and it’s the most high-level entity, which can be configured. ","date":"2020-10-02","objectID":"/tech-elasticsearch-1/:5:1","tags":["Technical Articles","Elasticsearch"],"title":"Getting familiar with Elasticsearch","uri":"/tech-elasticsearch-1/"},{"categories":["Tech"],"content":"2. Node: A node is a running instance of elasticsearch which belongs to a cluster. ","date":"2020-10-02","objectID":"/tech-elasticsearch-1/:5:2","tags":["Technical Articles","Elasticsearch"],"title":"Getting familiar with Elasticsearch","uri":"/tech-elasticsearch-1/"},{"categories":["Tech"],"content":"3. Document: The document is a non-relational data storage format. ","date":"2020-10-02","objectID":"/tech-elasticsearch-1/:5:3","tags":["Technical Articles","Elasticsearch"],"title":"Getting familiar with Elasticsearch","uri":"/tech-elasticsearch-1/"},{"categories":["Tech"],"content":"4. Index: An index is a logical namespace, it’s like a ‘database’ we use an RDBMS. It has a mapping definition provided at creation time which defines multiple types of data that can be stored in it. They mappings provide all information about the keys of the documents and the expected types for its value. ","date":"2020-10-02","objectID":"/tech-elasticsearch-1/:5:4","tags":["Technical Articles","Elasticsearch"],"title":"Getting familiar with Elasticsearch","uri":"/tech-elasticsearch-1/"},{"categories":["Tech"],"content":"5. Mapping: A mapping is like a ‘schema definition’ we do in RDBMS. Each index has a mapping, which defines each field data type within the index, plus several index-wide settings. A mapping can either be defined explicitly, or it will be generated automatically when a document is indexed. ","date":"2020-10-02","objectID":"/tech-elasticsearch-1/:5:5","tags":["Technical Articles","Elasticsearch"],"title":"Getting familiar with Elasticsearch","uri":"/tech-elasticsearch-1/"},{"categories":["Tech"],"content":"6. Shard: At times when an index can’t just be stored in a single node due to hardware challenges, we split it into x set of documents which is called sharding, and the split-up data is called a shard. A shard is an unbreakable entity in Elasticsearch, in the sense that a shard can only stay on one machine (Node). An index which is a group of shards can spread across multiple machines(ES nodes) but shards can not. Distributed Document Store, Notice that a document can't be further broken down to store it in multiple shards. ","date":"2020-10-02","objectID":"/tech-elasticsearch-1/:5:6","tags":["Technical Articles","Elasticsearch"],"title":"Getting familiar with Elasticsearch","uri":"/tech-elasticsearch-1/"},{"categories":["Tech"],"content":"Basic API requests Elasticsearch by default runs on 9200 port. So after installation, and starting your elasticsearch service, you can run a curl command on your localhost 9200 where Elasticsearch returns it’s running cluster details. curl http://localhost:9200/?pretty cURL: cURL is a command-line tool to transfer data to or from a server. We use it here to make requests to the Elasticsearch server. This will give you an output like below, mentioning version and other details. Getting an output here means you have an up and running elasticsearch cluster. And we are good to go. Shows if elasticsearch is up and running ","date":"2020-10-02","objectID":"/tech-elasticsearch-1/:6:0","tags":["Technical Articles","Elasticsearch"],"title":"Getting familiar with Elasticsearch","uri":"/tech-elasticsearch-1/"},{"categories":["Tech"],"content":"Creating an Index As, I said index is like a database, and we can have multiple indices in a cluster. So for storing some data we first need to add an index. An Index should be generally created for a document which is a flat data representation for a set of values, like Employee Index, Users Index, Posts Index. We first plan for what features we require, and as per that we broadly think of what indices we need. As per the data size, we decide whether 2 indices can be merged, to provide us with a more flat structure or whether an index can be broken into different indices if the use cases are as such that they have a separate concern. For creating an index you can send a put request to elasticsearch, with the index_name(example-index here) in the URL which you would like to create. curl -X PUT \"localhost:9200/example-index\" To view all the indices we can do a simple get request as curl -X GET \"localhost:9200/_cat/indices?v\" Creating an index ","date":"2020-10-02","objectID":"/tech-elasticsearch-1/:6:1","tags":["Technical Articles","Elasticsearch"],"title":"Getting familiar with Elasticsearch","uri":"/tech-elasticsearch-1/"},{"categories":["Tech"],"content":"Creating an Index with a mapping: Mapping can be auto-assigned by elasticsearch when you add a document, but I would advice to always create your own mapping, which is the definition of what keys will be stored in the document, and what will be there values. Having a consistent type ensures that we know which fields we can sort on, which fields we can filter on and much more. Below I am creating an index named example-index-with-mapping and I am defining some fields. Integer fields: The field defined as \"id\": { \"type\": \"integer\" }, defines id as an integer type field, that is the value which can be stored there is a numeric value. Text field: \"file_name\": { \"type\": \"text\" }, defines that the field file_name is of a text datatype, a text datatype will let you store all kinds and sizes of textual data, the size limits are huge and you can actually store megabytes of data in text fields, and they will be searchable too. Keyword field: \"name\": { \"type\": \"keyword\" }, defines a keyword field name, keyword fields are also string, but they are for small contents on which you would perform a sort, or a filter. Like when you want to filter all students with a Bachelor in Engineering education background. Examples of fields you may want to set as a Keyword can be emails, education, city, country, hostnames, tags etc. Nested field: \"resume\": { \"type\": \"nested\", \"properties\": { \"id\": { \"type\": \"integer\" }, \"file_name\": { \"type\": \"text\" } } },, defines a nested field resume, a nested field is one which defines another document inside it. Here resume is defined as a nested document, with id and file_name as it’s fields. A nested field is an array of documents inside the document. Object field \"education\": { \"properties\": { \"id\": { \"type\": \"integer\" }, \"name\": { \"type\": \"keyword\" } } }, defines education as an Object datatype, note that the way education is defined is so similar to the nested field, the only difference is that we haven’t mentioned type nested. An object field is used to attach a single document to a document. Inner objects are called object fields. They are not a set of documents like a nested field. In other words, the Nested field represents an array of objects and the object field represents a single object. Other important field types include date and booleans. See here for reading about all data types curl -X PUT \"localhost:9200/example-index-with-mapping?pretty\" -H 'Content-Type: application/json' -d ' { \"mappings\": { \"_doc\":{ \"properties\": { \"id\": { \"type\": \"integer\" }, \"first_name\": { \"type\": \"text\" }, \"last_name\": { \"type\": \"keyword\" }, \"resume\": { \"type\": \"nested\", \"properties\": { \"id\": { \"type\": \"integer\" }, \"file_name\": { \"type\": \"text\" } } }, \"education\": { \"properties\": { \"id\": { \"type\": \"integer\" }, \"name\": { \"type\": \"keyword\" } } } } } } }' Creating an Index with mapping ","date":"2020-10-02","objectID":"/tech-elasticsearch-1/:6:2","tags":["Technical Articles","Elasticsearch"],"title":"Getting familiar with Elasticsearch","uri":"/tech-elasticsearch-1/"},{"categories":["Tech"],"content":"Indexing a Document / Adding a document to the Index Now when we have a mapping set and an index created, we can add some documents. In the URL in the code below you will see the index name example-index-with-mapping and type _doc (note: types are deprecated, I have used it in the doc since I had an ES version 6 running, you can remove all it’s occurrences if you are using version 7 or later and rest will be good), after the type you see /1, this is the id which I want to create a document at. Yes we need to provide id in the URL, and ES will either create a document or update it depending on whether the id is existing or not. You can notice that we send a simple object containing the details of whichever field we want to add, there is no compulsion that we send values for all fields which we defined in the mapping. curl -X PUT \"localhost:9200/example-index-with-mapping/_doc/1?pretty\" -H 'Content-Type: application/json' -d ' { \"education\": [ {\"id\":\"1\",\"name\":\"BTech\"}, {\"id\":\"2\",\"name\":\"HS\"} ], \"first_name\": \"Rahul\", \"last_name\":\"Shah\" }' Indexing a document in the elasticsearch index ","date":"2020-10-02","objectID":"/tech-elasticsearch-1/:6:3","tags":["Technical Articles","Elasticsearch"],"title":"Getting familiar with Elasticsearch","uri":"/tech-elasticsearch-1/"},{"categories":["Tech"],"content":"Searching our document Searching for a text in the index is about writing queries, here I have a basic match query which aims to find documents where the last_name field has the word “Shah”. You can see in the output we get back the document which has that last_name. curl -X GET \"localhost:9200/example-index-with-mapping/_search?pretty\" -H\\ 'Content-Type: application/json' -d ' { \"query\": { \"match\": { \"last_name\": { \"query\": \"Shah\" } } } }' Searching the text Shah in our index ","date":"2020-10-02","objectID":"/tech-elasticsearch-1/:6:4","tags":["Technical Articles","Elasticsearch"],"title":"Getting familiar with Elasticsearch","uri":"/tech-elasticsearch-1/"},{"categories":["Tech"],"content":"Final Words In the post I have fairly touched a few important concepts which are required in working with Elasticsearch. In real life we don’t send raw requests to elasticsearch service as shown above, we use elasticsearch SDK, and doing things becomes much more simpler than above. There are much more to elasticsearch, from queries to aggregations to custom techniques of indexing data. I find the elasticsearch documentation very descriptive, so I won’t be looking at writing posts on queries as such. But I will try to write some on simplifying your queries and customizing indexes and working with scripts etc, until then adios amigos. ","date":"2020-10-02","objectID":"/tech-elasticsearch-1/:7:0","tags":["Technical Articles","Elasticsearch"],"title":"Getting familiar with Elasticsearch","uri":"/tech-elasticsearch-1/"},{"categories":["Tech"],"content":"Shayan writes about his experience with Web Scraping he had recently","date":"2020-10-01","objectID":"/guest-1/","tags":["Python","Technical Articles","Web Scraping"],"title":"Web Scraping","uri":"/guest-1/"},{"categories":["Tech"],"content":"Did you know that web scraping is integral to the data analysis process? It is because it allows quick and efficient extraction of data from different sources, which can then be processed to glean insights as required. Data Analysis is a huge market today, for some of the software giants it’s the source of every decision making, even for small businesses, they get to keep track of the brand value and reputation of their company. You never know when a small automation script to dig in data from various sources might find someplace in engineering life or workspace 😄 (ignore the emoticon, just trying to get your attention). In this post, I will be using python, it’s a diverse programming language and when you pick the right tools, libraries, you are a superhero. Starting to learn any programming language takes curiosity, will, time, and a couple of energy drinks 😉, I would suggest you getting the basic concept of Python handy if you are not already using it. ","date":"2020-10-01","objectID":"/guest-1/:0:0","tags":["Python","Technical Articles","Web Scraping"],"title":"Web Scraping","uri":"/guest-1/"},{"categories":["Tech"],"content":"Hang on with me for 15 mins and scrape your first website today. ","date":"2020-10-01","objectID":"/guest-1/:0:1","tags":["Python","Technical Articles","Web Scraping"],"title":"Web Scraping","uri":"/guest-1/"},{"categories":["Tech"],"content":"The Basics of web scraping The crawler A web crawler, which we generally call a “spider,” is an artificial intelligence program that browses the internet to index and searches for content by following links and exploring layers of pages, like a person with too much time on their hands. It’s the machine version of an internet stalker. The Scrapper A web scraper is a specialized tool designed to accurately and quickly extract data from a web page. Web scrapers vary widely in design and complexity, depending on the project. What's the difference in a scrapper and a crawler ","date":"2020-10-01","objectID":"/guest-1/:0:2","tags":["Python","Technical Articles","Web Scraping"],"title":"Web Scraping","uri":"/guest-1/"},{"categories":["Tech"],"content":"Prerequisite An IDE, I prefer Spyder Python3, comes pre-installed on Ubuntu, and for another OS, just have it setup. and Test if you can view the Python INteractive Shell by running python3 Basic knowledge of HTML tags and python ","date":"2020-10-01","objectID":"/guest-1/:0:3","tags":["Python","Technical Articles","Web Scraping"],"title":"Web Scraping","uri":"/guest-1/"},{"categories":["Tech"],"content":"OK!!! Let’s start Step 1: Import the required libraries we need BeautifulSoup from bs4 (It’s a simple library that makes your scraping easy) Request and urlopen from urllib from bs4 import BeautifulSoup from urllib.request import Request, urlopen Step 2: Get the Url you want to scrape data from. url = \"abc.com\" Step 3: making a request to access resources using the request module req = Request(url+\"-\"+str(a), headers={'User-Agent': 'Mozilla/5.0'}) Note: Take care of the user agent. Sometimes when you try to scrape a website, you are barred from it because the safety protocol does not allow you to do so. There are many automated scrapper scripts continuously running to scrape a website. Robots.txt is a text file that the webmasters create to instruct web robots (typically search engine robots) how to crawl pages on their website, and who all are allowed to crawl. So playing around with the user agent can help. Step 4: Creating soup object to parse HTML script html_doc = urlopen(req).read() soup=BeautifulSoup(html_doc,'html.parser') You can see the beautiful soup object print(soup.prettify).Which represents the document as a nested data structure Step 5: Use a predefined function to extract your desired information. Open the web page you wanted to scrape, and inspect it using Ctrl+Shift+I key or right-click and select [Inspect]. Now you can see the HTML tags using which you can easily navigate through containers and the data you want to extract. Here are some simple ways to navigate the HTML with soup soup.title returns the title of the context. Another common task is extracting all the text from a page: soup.get_text() soup.a returns the link in the context. \u003ca\u003e tags represents links in HTML.soup.find_all('a')functions returns a list of all the links in the soup object. [\u003ca class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\"\u003eElsie\u003c/a\u003e, \u003ca class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\"\u003eLacie\u003c/a\u003e, \u003ca class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\"\u003eTillie\u003c/a\u003e] You can also call get(‘href’) to get links from the anchor (\u003ca\u003e) tag. for link in soup.find_all('a'): print(link.get('href')) output http://example.com/elsie http://example.com/lacie http://example.com/tillie ","date":"2020-10-01","objectID":"/guest-1/:0:4","tags":["Python","Technical Articles","Web Scraping"],"title":"Web Scraping","uri":"/guest-1/"},{"categories":["Tech"],"content":"Final Words It is very straightforward to extract data using beautiful soup. After which you need to use data structures such as the data frame in the pandas’ library which is a very famous one to store the extracted data in a well-structured way. It’s a simple application of python that you can learn if you are a college fresher and this also gives you some hold on python programming and confidence about how basic the coding world is. I wrote a web-scrapper sometimes back, to extract information from Naukri( a job portal), which scrapes the Job vacancies listed on Naukri’s website in an excel sheet. After which we worked on analyzing what languages are more in demand. Here is the link to my github repository Hope you enjoyed the post!!! Guest Post Hey, It’s me Faizaan, I would like to thank Shayan for trying to put his work from college into words for other students who need a little push to start with python and data analytics. I am Looking forward to having him here and sharing his experiences. ","date":"2020-10-01","objectID":"/guest-1/:0:5","tags":["Python","Technical Articles","Web Scraping"],"title":"Web Scraping","uri":"/guest-1/"},{"categories":["Tech"],"content":"I show how to configure dynamoDB in dev environment and discuss some of the basic API functionality of DynamoDB in this post","date":"2020-09-12","objectID":"/tech-dynamo-2/","tags":["AWS","Technical Articles","DynamoDB"],"title":"Configuring DynamoDB and basic operations for ruby apps","uri":"/tech-dynamo-2/"},{"categories":["Tech"],"content":"Intro Hi, in this blog post I’m discussing some of the basic API functions which come in handy to use dynamoDB. These are all well documented on Amazon documentation as well as the library which might use to integrate with AWS, So why this post ? The reason for that is there’s a lot of options, and to help you with what you need in the beginning here I list my opinions and my suggested way of doing it. Remember working on a DynamoDB is almost a no-brainer when you got your basics clear. If you are a beginner and you know nothing about no SQL databases, I have a basic introduction post where I talk about most of the if’s and but’s you need to know and a simple guide on the concepts DynamoDB here. Here my choice of language is Ruby, but it’s an easy digest for any other as well. Note for rails users For rails users, DynamoDB is not the right choice for ORM in my opinion, but you might have a different one. If you are looking for using DynamoDB with your Active Record here’s the best you got with this gem called Dynamoid. If that’s not what you need Dynamo for this is the right place. ","date":"2020-09-12","objectID":"/tech-dynamo-2/:1:0","tags":["AWS","Technical Articles","DynamoDB"],"title":"Configuring DynamoDB and basic operations for ruby apps","uri":"/tech-dynamo-2/"},{"categories":["Tech"],"content":"Setting up a local development environment Download a locally executable DynamoDB application which you can use for development from aws Navigate to the folder and run java -Djava.library.path=./DynamoDBLocal_lib -jar DynamoDBLocal.jar -help, use any option, if you want to. I use the inMemory flag, to not persist data locally and let it run just on memory, and the sharedDB for storing all-region work at one place, good for saving on some system requirements. The flags are well described here java -Djava.library.path=./DynamoDBLocal_lib -jar DynamoDBLocal.jar -sharedDb -inMemory And Hurray, your local DB server is running. Quickly download awscli if you don’t have one. awscli provides you with the superpowers to communicate with AWS from command line :p. Linux Users: sudo apt install awscli Mac Users: brew install awscli (Add Homebrew if you don’t have it 🙂 Alternatively, take some time and read about cli here Now that we have aws-cli and dynamoDB client running on the local machine. We can go ahead and run a command. Let’s see what all tables exists, we expect the output should be empty. But Before that we need to configure AWS credentials, don’t worry we only need fake values for the local system to work. Run aws configure Add any fake string as shown in the image below, for access key id and access key secret. Set an AWS region, some examples include, us-west-2, ap-south-1. Set CLI response type to JSON, or whatever your choice is if you read the CLI documentation. We have a local instance set, credentials added for aws-cli, and now we should be able to make a call to our local dynamoDB. aws dynamodb list-tables --endpoint-url http://localhost:8000 aws configure example (Click to Expand) That’s all for local setup and using API endpoints through CLI. You can update your credentials to real one, and not pass an endpoint URL in a general case, to use the AWS dynamoDB which is on the cloud. If due to some config issue it doesn’t fetch data from the table you want, you might need to pass an end-point URL, and for that you can follow the standard scheme for AWS service URLs. Aws service URL structure is: protocol://service-code.region-code.amazonaws.com. So for a dynamoDB table in us-west-2, we will have the endpoint as https://dynamodb.us-west-2.amazonaws.com. ","date":"2020-09-12","objectID":"/tech-dynamo-2/:2:0","tags":["AWS","Technical Articles","DynamoDB"],"title":"Configuring DynamoDB and basic operations for ruby apps","uri":"/tech-dynamo-2/"},{"categories":["Tech"],"content":"Setting up gems and client configs on any ruby application Here, I will show you how to configure your ruby app to use dynamoDB. Rails users can add specific classes and codes as per their choices in initializers and concerns etc. We will use the gem aws-sdk-dynamodb, so create a Gemfile, and add the latest one from RubyGems, and bundle. At the time of writing this I’m using gem 'aws-sdk-dynamodb', '~\u003e 1.0.0.rc7' # In Gemfile source 'https://rubygems.org' gem 'aws-sdk-dynamodb', '~\u003e 1.0.0.rc7' Now we will create an AWS dynamoDB client object, such that it can be used to perform the actions. In the most basic syntax this is as simple as calling new on Aws::DynamoDB::Client class with a region option passed. And if you are in an environment with a preset AWS config, it is all you need. def client @client ||= Aws::DynamoDB::Client.new(region: \"us-west-2\") end Ok, we can define the aws global configs for this app if required as below, assuming credentials struct is from the environment. And that should be good to go for using client. Aws.config.update( access_key_id: credentials.aws[:access_key_id], secret_access_key: credentials.aws[:secret_access_key], region: credentials.aws[:region] ) We can also pass, the config key-value pairs to the Aws::DynamoDB::Client class while initializing it, and that will also be good enough. Since through this post we will be using local dynamoDB, we only need to pass a region and our local endpoint URL. def client @client ||= Aws::DynamoDB::Client.new(region: 'us-west-2', endpoint: 'http://localhost:8000') end By now, we have added the required gem, and have a client object. This looks like the right time to create a table. So let’s go ahead and do that. ","date":"2020-09-12","objectID":"/tech-dynamo-2/:3:0","tags":["AWS","Technical Articles","DynamoDB"],"title":"Configuring DynamoDB and basic operations for ruby apps","uri":"/tech-dynamo-2/"},{"categories":["Tech"],"content":"Working with tables and records ","date":"2020-09-12","objectID":"/tech-dynamo-2/:4:0","tags":["AWS","Technical Articles","DynamoDB"],"title":"Configuring DynamoDB and basic operations for ruby apps","uri":"/tech-dynamo-2/"},{"categories":["Tech"],"content":"Creating a table I hope you have been through my last post on dynamoDB and you have already created a table using the AWS web console. Here we will be doing the same but from the SDK. In general, you can add these code in a rails task, or a ruby script and run it when you require to setup. Also, many tend to use the web console to create the tables and use them since it’s a one-time affair, but it is important for them who are adding it to an application and want to use for different environments etc. We will use create_table API endpoint. Here we need to understand a few basic options which we will be sending in params. table_name: Table name 🙂 key_schema: Key schema is an array of objects containing information about fields which you want as your primary key. attribute_name: It’s the field name for which you are setting a primary key attribute key_type: It defines the role of the key attribute set, Hash defines a partition key and range defines a sort key. attribute_definitions: Here we need to define the data type of the attributes we are setting in key schemas for the table and any indexes which other than our primary index too. The data types for the attributes are: S - the attribute is of type String N - the attribute is of type Number B - the attribute is of type Binary provisioned_throughput: provisioned throughput is about what kind of capacity you wish to have and will pay for. We don’t need to jump into understanding the exact need we have, AWS is tricky here and it’s a whole bunch of a subject. If you need to understand how the billing works for the provisioned system you can start with this. In very naive terms we can understand this as below: RCU: Read capacity unit represents a read request for an item of 4KB size in 1 second. WCU: Write capacity unit represents a write request for an item of 1KB size in 1 second. # main.rb require 'aws-sdk-dynamodb' def create_test_table params = { table_name: \"test-dev\", key_schema: [ { attribute_name: \"id\", key_type: \"HASH\" # Partition Key }, { attribute_name: \"submitted_at\", key_type: \"RANGE\" # Sort Key } ], attribute_definitions: [ { attribute_name: \"id\", attribute_type: \"S\" }, { attribute_name: \"submitted_at\", attribute_type: \"N\" } ], provisioned_throughput: { read_capacity_units: 1, write_capacity_units: 1 } } result = client.create_table(params) puts \"Status: \" + result.table_description.table_status rescue Aws::DynamoDB::Errors::ServiceError =\u003e e puts \"Unable to create table: #{e.message}\\n\" end def client @client ||= Aws::DynamoDB::Client.new(region: 'us-west-2', endpoint: 'http://localhost:8000') end create_test_table After running the above code, you should be able to see your table listed using the call we made earlier. aws dynamodb list-tables --endpoint-url http://localhost:8000 creating a table example ","date":"2020-09-12","objectID":"/tech-dynamo-2/:4:1","tags":["AWS","Technical Articles","DynamoDB"],"title":"Configuring DynamoDB and basic operations for ruby apps","uri":"/tech-dynamo-2/"},{"categories":["Tech"],"content":"Updating a table We can not update a table’s primary keys, but we can add global indexes with table update calls, update rcu, wcu, and perform other updates like enabling time_to_live. Yes, I am taking this opportunity to tell you something about time to live. Dynamo provides us with an option to set time to live for all the records we create in epoch(a kind of time format). Time to live is a great tool for apps that are storing data on dynamo temporarily. You can set it ttl (time_to_live) to given date time and the record will be deleted by dynamo when the time comes. For updates aws provides update_table, which can be used for updating rcu, wcu and adding GSI (global secondary index). Here I will use anothe api method update_time_to_live. Here, I am enabling time to live specification and also setting the attribute name, which will allow us to add a epoch value for the records time to live. def update_table result = client.update_time_to_live( { table_name: 'test-dev', time_to_live_specification: { enabled: true, attribute_name: \"ttl\" } } ) if result.time_to_live_specification.enabled puts \"TTL enabled\" end rescue Aws::DynamoDB::Errors::ServiceError =\u003e e puts e.message.to_s end ","date":"2020-09-12","objectID":"/tech-dynamo-2/:4:2","tags":["AWS","Technical Articles","DynamoDB"],"title":"Configuring DynamoDB and basic operations for ruby apps","uri":"/tech-dynamo-2/"},{"categories":["Tech"],"content":"Writing records on the table Writing records on the table is yet another simple task, you can use the below code and pass your record to add. What we are doing below is, we are calling put_item method with item and table name. Item consists of an object which can be multiple levels nested or flat, but must have the primary key attributes defined. As you already know, we can try new attributes on the fly and we only need to stick with the primary key attributes, others are all dynamically created. put_item will add an item or replace the item if there’s a primary key conflict. def create_record(item) client.put_item( table_name: 'test-dev', return_consumed_capacity: \"TOTAL\", item: item ) rescue Aws::DynamoDB::Errors::ServiceError =\u003e e puts e.message.to_s end create_record({ id: 12, submitted_at: 12-10-2020, name: 'John Doe', degree: 'BE' }) ","date":"2020-09-12","objectID":"/tech-dynamo-2/:4:3","tags":["AWS","Technical Articles","DynamoDB"],"title":"Configuring DynamoDB and basic operations for ruby apps","uri":"/tech-dynamo-2/"},{"categories":["Tech"],"content":"Fetching records from the table We can fetch a record by calling get_item, and we need to pass the primary key attributes (Both partition and sort key if both are created, or just partition key if there’s only one primary key attribute). Let’s use the get_item method and fetch the data we created just now. def fetch_record(key) response = client.get_item(table_name: 'test-dev', key: key) response.item rescue Aws::DynamoDB::Errors::ServiceError =\u003e e puts e.message end fetch_record({ id: 12, submitted_at: 12-10-2020}) ","date":"2020-09-12","objectID":"/tech-dynamo-2/:4:4","tags":["AWS","Technical Articles","DynamoDB"],"title":"Configuring DynamoDB and basic operations for ruby apps","uri":"/tech-dynamo-2/"},{"categories":["Tech"],"content":"Updating a record on the table We can use put_item and replace the entire record, or we can also just update a non-primary key attribute. Here we need to use expressions to make the update call using update_item. DynamoDB provides us with some expressions like an SQL syntax we use, one of which is used here and is set field = :the_variable_which_we_will_use_in_expression_attr_values. In update_expression we are passing a string that resembles a query to let dynamo identify a field with a specific name, which we will use in the expression_attribute_values to set a value. In return_values option we are setting UPDATED_NEW to get back the updated data. def update_record(key, field, new_value) client.update_item({ table_name: 'test-dev', key: key, update_expression: \"set #{field} = :field\", expression_attribute_values: { ':field' =\u003e new_value }, return_values: 'UPDATED_NEW' }) rescue Aws::DynamoDB::Errors::ServiceError =\u003e e puts e.message end ","date":"2020-09-12","objectID":"/tech-dynamo-2/:4:5","tags":["AWS","Technical Articles","DynamoDB"],"title":"Configuring DynamoDB and basic operations for ruby apps","uri":"/tech-dynamo-2/"},{"categories":["Tech"],"content":"Deleting a Table Just to finish-up. Delete is as simple as expected. def delete_table client.delete_table({ table_name: 'test-dev' }) puts \"Table deletion process started, aws will delete it soon\" rescue Aws::DynamoDB::Errors::ServiceError =\u003e e puts \"Unable to delete table: #{e.message}\\n\" end ","date":"2020-09-12","objectID":"/tech-dynamo-2/:4:6","tags":["AWS","Technical Articles","DynamoDB"],"title":"Configuring DynamoDB and basic operations for ruby apps","uri":"/tech-dynamo-2/"},{"categories":["Tech"],"content":"Some other APIs to look at In the last section we saw some basic scaffolding for executing simple calls to dynamoDB. It can be of good use, and other than that we also have few other important APIs to look at. I will just describe them, and you can have a brief understanding of them, such that you know what to use, when you need it. Batch APIs BatchGetItem: It can be used to fetch items from different tables using Partition Key and Sort Key. In a single BatchGetItem call, we can fetch up to 16MB data or 100 items. BatchWriteItem: It can be used to delete or put items on one or more tables in DynamoDB in one call. We can write up to 16 MB data, which can be 25 put and delete requests. Query and Pagination APIs Query: Query operation will return all items that are matched with the partition key of the table. Sort Key is further useful to filter and sort items but it is optional. Scan: Scan operation doesn’t require Partition Key or Sort Key to fetch results from the table. As the name suggests, it scans an entire table to filter results based on attribute values passed as filters. Pagination: DynamoDB Query/Scan results return a maximum of 1MB response size so if our request matches an item of size more than 1MB, it gets automatically divided into pages. In such cases DynamoDB returns a special response parameter “LastEvaluatedKey” and this can be used to fetch next page results. Please note we need to pass the value of “LastEvaluatedKey” as “ExclusiveStartKey” in the next request to DynamoDB. ","date":"2020-09-12","objectID":"/tech-dynamo-2/:5:0","tags":["AWS","Technical Articles","DynamoDB"],"title":"Configuring DynamoDB and basic operations for ruby apps","uri":"/tech-dynamo-2/"},{"categories":["Tech"],"content":"Secondary Indexes DynamoDB is very bad with fetching data when you are not using a key to get the data, and instead doing a scan operation. It was not meant to do that ever, if you need search capabilities elastic search is your goto option. We can query on primary indexes and can get data, but when we know that we will be needing some other attributes pairs too for some data fetching we can go ahead, and add them as indexes. Dynamo provides us with two types of secondary indexes. Global secondary index – An index with a partition key and sort key that can be different from those on the table. They are stored detached from the main table with their own partition. Local secondary index – An index that has the same partition key as the table, but a different sort key. They are stored with the table, in the same partition as the data it is referencing to. ","date":"2020-09-12","objectID":"/tech-dynamo-2/:6:0","tags":["AWS","Technical Articles","DynamoDB"],"title":"Configuring DynamoDB and basic operations for ruby apps","uri":"/tech-dynamo-2/"},{"categories":["Tech"],"content":"End words You can start working now, you have enough knowledge about the tools and and it’s always the journey where you learn, I hope I was able to stick to the very basics and help you out in understanding them. Adios. ","date":"2020-09-12","objectID":"/tech-dynamo-2/:7:0","tags":["AWS","Technical Articles","DynamoDB"],"title":"Configuring DynamoDB and basic operations for ruby apps","uri":"/tech-dynamo-2/"},{"categories":["Tech"],"content":"I discuss the bare minimum concepts one needs to start working with DynamoDB, or to select DynamoDB for their application","date":"2020-08-30","objectID":"/tech-dynamo-1/","tags":["AWS","Technical Articles","DynamoDB"],"title":"Bare minimum approach on learning AWS DynamoDB","uri":"/tech-dynamo-1/"},{"categories":["Tech"],"content":"Why this post ? I am working with DynamoDB for some time now and on that journey I have explored about things that I would like to share. I am no master at Dynamo and nor does one needs to be for being able to utilize it’s capacity. Here are most of the bare minimum things you need to know before jumping into starting to integrate DynamoDB to your application or before you chose it as a viable option. The post is pretty basic, and you can skip topics if you are well aware of them. ","date":"2020-08-30","objectID":"/tech-dynamo-1/:1:0","tags":["AWS","Technical Articles","DynamoDB"],"title":"Bare minimum approach on learning AWS DynamoDB","uri":"/tech-dynamo-1/"},{"categories":["Tech"],"content":"What’s a NoSQL aka Not Only SQL Database System ? A NoSQL database provides you with the flexibility in managing data where you ideally don’t need to stick to a predefined schema for tabular storage. While NoSQL databases have stayed with us for a long time now, they became a thing in the era of cloud, big data, and high volume web and mobile applications. NoSQL databases are generally linked to: High Scalability High Availability Big Data Storage Capacity Easy Replication Fast Performance The above points sound so cool, but it’s not entirely true for all the NoSQL databases and there are many more variables and architectural choices made which provides you with them. One mantra for NoSQL which I remind myself whenever I am switching from a relational system is, NoSQL is a denormalized system and I must not think of the DB structure in a relational system. Try to flatten the data, and don’t worry much if there’s a few data duplication. Like you don’t go ahead and make a table to store schools when a student table is linked with a school. ","date":"2020-08-30","objectID":"/tech-dynamo-1/:2:0","tags":["AWS","Technical Articles","DynamoDB"],"title":"Bare minimum approach on learning AWS DynamoDB","uri":"/tech-dynamo-1/"},{"categories":["Tech"],"content":"Some of the common categories of NoSQL databases Key-Value Store: They are essentially big hash tables of keys and values( Common examples are Dynamo DB and Redis). Document-based Store: The documents are made up of tagged elements(Common examples include MongoDB and CouchDB). Column-Based Store: They have storage blocks and each storage block contains data from only one column(HBase, Cassandra) Graph-Based Store: They are network-based, and uses edges and nodes to store/represent data(SAP Hana, Neo4J) I know that’s pretty boring stuff when you just read it, so I would advise to go ahead and see some of the data examples, of how they are stored and everything, will be clearer. An Example of Data stored in a Key-Value Store: { \"CarType\": \"Sedan\", \"Name\": \"Maserati Ghibli\", \"Price\": \"$10,000\" \u003c..other attributes\u003e }, { \"CarType\": \"Hatchback\", \"Name\": \"Volkswagon Polo\", \"Price\": \"$5,000\" \u003c..other attributes\u003e } ","date":"2020-08-30","objectID":"/tech-dynamo-1/:3:0","tags":["AWS","Technical Articles","DynamoDB"],"title":"Bare minimum approach on learning AWS DynamoDB","uri":"/tech-dynamo-1/"},{"categories":["Tech"],"content":"How to select the right DB to use ? Though we already have a selection of DynamoDB in this post here, it’s of great importance that you know what you are using and why. What comes in handy for choosing what you need is the popular CAP theorem by Eric Brewer for distributed systems. Since NoSQL databases are in generally largely distributed systems, people have used it in the selection of their NoSQL DB. Coming to the theorem, it states that a networked shared data systems can only guarantee/strongly support two of the following three properties: Consistency — A guarantee that every node in a distributed cluster returns the same, most recent, successful write. Availability — Every non-failing node returns a response for all read and write requests in a reasonable amount of time. Partition Tolerant — The system continues to function and upholds its consistency guarantees in spite of network partitions. Here’s an image which is a nice depiction of the above 3 properties. You can see that, DynamoDB provides us with Availability and Partition Tolerance, but doesn’t guarantee Consistency. Although as of 2020 AWS has a strongly consistent and transactional consistent model, where AWS promises data consistency, it’s not the best player in that area. CAP Theorem (Click to Expand) ","date":"2020-08-30","objectID":"/tech-dynamo-1/:4:0","tags":["AWS","Technical Articles","DynamoDB"],"title":"Bare minimum approach on learning AWS DynamoDB","uri":"/tech-dynamo-1/"},{"categories":["Tech"],"content":"Are you still unsure ? Here’s a list of questions, I copied from AWS documentation 😉 These are just enough to decide if all the “Gyaan” above didn’t work. Can you organize your data in hierarchies or an aggregate structure in one or two tables? Are traditional backups impractical because of table update rate or overall data size? Does your database workload vary significantly by time or have high-traffic events? Does your application or service consistently require response time in the single milliseconds? Do you need to provide services in a scalable, replicated configuration? Does your application need to store data in the high-terabyte size range? Are you willing to invest in a short but possibly steep NoSQL learning curve? Is data protection important? Those questions are really the marketing ones. But hey, AWS promises you something. I will add another one: Are you on AWS cloud services already? You can obviously use it from other services like a GCP, but that won’t be secure on the go. ","date":"2020-08-30","objectID":"/tech-dynamo-1/:5:0","tags":["AWS","Technical Articles","DynamoDB"],"title":"Bare minimum approach on learning AWS DynamoDB","uri":"/tech-dynamo-1/"},{"categories":["Tech"],"content":"Introducing the already introduced DynamoDB Amazon DynamoDB is a key-value based NoSQL database which is highly scalable, reliable, fully managed and durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications By fully managed they mean, you don’t need to worry about the underlying hardware, servers, or operating system. Data stored in the DynamoDB is redundantly copied across multiple Availability Zones and thus it protects data loss due to underlying hardware failures by default, which is a big thing. In simple terms, you can say that DynamoDB can handle a large amount of data and large traffic without you worrying about the architecture behind, and can be integrated to your application like butter with the AWS ecosystem. An example usage of DynamoDB in the AWS Ecosystem ","date":"2020-08-30","objectID":"/tech-dynamo-1/:6:0","tags":["AWS","Technical Articles","DynamoDB"],"title":"Bare minimum approach on learning AWS DynamoDB","uri":"/tech-dynamo-1/"},{"categories":["Tech"],"content":"Data Organisation and Modelling DynamoDB organizes data as tables, yes tables. Each table contains several items in each row, and each item has keys and values for each column. UUID (Primary Key) StudentName MathsScore BioScore AccountsScore 1212-3322-5861-9865 Ram 59 98 1582-2249-9513-6538 Shyam 79 89 In the example above, one item can be represented in JSON data as: { \"UUID\": 1212-3322-5861-9865, \"StudentName\": 'Ram', \"MathsScore\": 59, \"BioScore\": 98 } What’s different about a DynamoDB table is that you have the freedom to select a primary key while keeping all other columns dynamic. So in the above example, you can keeping adding different kinds of scores or any other information. You just define the primary key and data can be added with a dynamic set of keys and values. ","date":"2020-08-30","objectID":"/tech-dynamo-1/:7:0","tags":["AWS","Technical Articles","DynamoDB"],"title":"Bare minimum approach on learning AWS DynamoDB","uri":"/tech-dynamo-1/"},{"categories":["Tech"],"content":"Primary Keys When you create a table in DynamoDB, in addition to the table name, you must specify the primary key of the table. The primary key uniquely identifies each item in the table, so that no two items can have the same key. A dynamo primary key is either based on a single field or a couple of fields. Partition key – A simple primary key, composed of one attribute known as the partition key. DynamoDB uses the partition key’s value as input to an internal hash function. The output from the hash function determines the partition (physical storage internal to DynamoDB) in which the item will be stored. Partition key and sort key - Referred to as a composite primary key, and as the name suggests it is composed of two attributes. The first attribute is the partition key, and the second attribute is the sort key. All items with the same partition key value are stored together, in sorted order by sort key value. Let’s add a sort key on the previous example: UUID (Partition Key) StudentName (Sort Key) MathsScore BioScore AccountsScore 1212-3322-5861-9865 Ram 59 98 1582-2249-9513-6538 Shyam 79 89 When you have a composite primary key, the uniqueness is checked on the pair of partition and sort keys, so in the above example we have UUID, but uniqueness is only checked on UUID and StudentName together as a unit, so one can also store another record with the same UUID and a different StudentName. The selection of a wise primary key is very important as the DB is indexed on your primary key and provides with a quick result when you try to find by an indexed field. And this selection is one of the places where your most of the thinking should go when you are modeling your data. ","date":"2020-08-30","objectID":"/tech-dynamo-1/:7:1","tags":["AWS","Technical Articles","DynamoDB"],"title":"Bare minimum approach on learning AWS DynamoDB","uri":"/tech-dynamo-1/"},{"categories":["Tech"],"content":"Data types supported are: Scalar Types : Number, String, Binary, Boolean and Null. Document Types : List and Map Set Types : Number Set, String Set, and Binary Set. You don’t need to define them while creating any item, it’s just for a reference on what data types can be stored. ","date":"2020-08-30","objectID":"/tech-dynamo-1/:7:2","tags":["AWS","Technical Articles","DynamoDB"],"title":"Bare minimum approach on learning AWS DynamoDB","uri":"/tech-dynamo-1/"},{"categories":["Tech"],"content":"Creating a table in AWS Console Visit the AWS console page. Select DynamoDB from the services menu and click on the create table option. You have a small form, asking for the table name and primary key. Use the defaut options for now and click on create Table button. When you are creating a table you can provide partition key and an optional sort key, an important point to note is that these columns can’t be changed. You can update the attributes of any item in dynamo, but can never update a key if you ever want to, what you do is you delete the item and then re-create another one with a different key. Creating a table in DynamoDB You will see a screen like below Existing table view Visit the items tab, there’s no item there, you will be using AWS API endpoints to fetch,create, update, delete items from your app but for now just go ahead and create your first item by clicking on Create Item 😄 Existing table view You can see how you can add other dynamic sets of keys and add their values on the go. Keep exploring the console options and you will get some hold on what it is. ","date":"2020-08-30","objectID":"/tech-dynamo-1/:8:0","tags":["AWS","Technical Articles","DynamoDB"],"title":"Bare minimum approach on learning AWS DynamoDB","uri":"/tech-dynamo-1/"},{"categories":["Tech"],"content":"How to get started ? Checkout AWS SDK for DynamoDB which is supported for several languages, and can be used to interact with DynamoDB API endpoints for basic DB operations seamlessly. You can also read my next post which will be more about the usage part of it. ","date":"2020-08-30","objectID":"/tech-dynamo-1/:9:0","tags":["AWS","Technical Articles","DynamoDB"],"title":"Bare minimum approach on learning AWS DynamoDB","uri":"/tech-dynamo-1/"},{"categories":["Tech"],"content":"Last words until the next one DynamoDB is one of the easiest no-sql databases to start working with, but as you keep exploring you can dive deeper into understanding its capabilities. In a follow-up article, I will be writing about indexes, read and write capacity units, API functions, and I will give a walkthrough on how you can use AWS APIs to create and manage a DyanmoDB table and show some CRUD operations. ","date":"2020-08-30","objectID":"/tech-dynamo-1/:10:0","tags":["AWS","Technical Articles","DynamoDB"],"title":"Bare minimum approach on learning AWS DynamoDB","uri":"/tech-dynamo-1/"},{"categories":["Tech"],"content":"References Here are a few links this article takes inspiration from. https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Introduction.html (AWS ecosystem example image) https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html https://medium.com/tensult/core-concepts-of-amazon-dynamodb-a265a3fc70a https://opensourceforu.com/2017/05/different-types-nosql-databases/ https://www.3pillarglobal.com/insights/exploring-the-different-types-of-nosql-databases https://www.mysoftkey.com/architecture/understanding-of-cap-theorem/ (CAP Theorem Diagram) https://www.geeksforgeeks.org/introduction-to-nosql/ ","date":"2020-08-30","objectID":"/tech-dynamo-1/:11:0","tags":["AWS","Technical Articles","DynamoDB"],"title":"Bare minimum approach on learning AWS DynamoDB","uri":"/tech-dynamo-1/"},{"categories":["personal"],"content":"Initial blogpost.","date":"2020-08-16","objectID":"/personal-1/","tags":["personal"],"title":"First Ever Blog Post","uri":"/personal-1/"},{"categories":["personal"],"content":"First Ever Blog Post Stepping into the writer’s world. Hi random visitor, you are either one of my friends I have forced to visit this place or someone who liked some work by the future me or maybe just the web crawler. In my pursuit of learning and trying out things, with my baby turtle steps, I have realized one thing, that feeling confident about something and being confident enough to explain other people are two very different things. Just like living a moment and being able to share other people how you felt about it. The later provides me with extreme joy and maybe as a human acceptance from people makes me happy about it. In general, I am no good with conversations, or writing, but it’s never too late right? So here’s me starting to share the experiences I have with creating something, configuring something, hating something, and living something. Wish me luck, and less procrastination. ","date":"2020-08-16","objectID":"/personal-1/:0:0","tags":["personal"],"title":"First Ever Blog Post","uri":"/personal-1/"},{"categories":null,"content":" What I’m doing now ","date":"2020-08-16","objectID":"/about/:0:0","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"As on 17th June 2023 ","date":"2020-08-16","objectID":"/about/:1:0","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"I have shifted to Bengaluru ","date":"2020-08-16","objectID":"/about/:1:1","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"I am travelling a lot and have started working harder on my personal and professional goals ","date":"2020-08-16","objectID":"/about/:1:2","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"My plans are to improve my DSA skills and look for more career opportunities ","date":"2020-08-16","objectID":"/about/:1:3","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"I have been on my learning path to handle a camera, and have been clicking some good photos ","date":"2020-08-16","objectID":"/about/:1:4","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"Still listening to all kinds of music. ","date":"2020-08-16","objectID":"/about/:1:5","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"As on 13th September 2020 ","date":"2020-08-16","objectID":"/about/:2:0","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"I am working on Ruby on Rails and React applications. ","date":"2020-08-16","objectID":"/about/:2:1","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"Getting my hands dirty with AWS cloud services. ","date":"2020-08-16","objectID":"/about/:2:2","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"Watching Netflix and procrastinating on Youtube on weekends. ","date":"2020-08-16","objectID":"/about/:2:3","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"Clicking photos of random things. ","date":"2020-08-16","objectID":"/about/:2:4","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"Listening to all kinds of music. ","date":"2020-08-16","objectID":"/about/:2:5","tags":null,"title":"","uri":"/about/"},{"categories":null,"content":"Waiting for the Corona Virus to go away. 😄 Checkout: NowNowNow ","date":"2020-08-16","objectID":"/about/:2:6","tags":null,"title":"","uri":"/about/"}]